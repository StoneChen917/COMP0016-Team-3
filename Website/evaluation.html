<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation</title>

    <link rel="stylesheet" href="assets/css/Global.css">
    <link rel="stylesheet" href="assets/css/Header.css">
    <link rel="stylesheet" href="assets/css/Footer.css">

    <link rel="stylesheet" href="assets/css/pages/index.css">

</head>

<body>
    <header>
        <!-- ***** Logo Start ***** -->
        <a href="index.html" class="logo">
            <img src="assets/images/go-logo-2020.png" alt="IFRC-GO">
        </a>
        <!-- ***** Logo End ***** -->
        <!-- ***** Menu Start ***** -->
        <ul class="menu" id="menu">
            <li class="submenu">
                <a class="mainlink" href="index.html">Home</a>
            </li>
            <li class="submenu">
                <a class="mainlink" href="requirements.html">Requirements</a>
            </li>

            <li class="submenu" class="submenu">
                <a class="mainlink" href="research.html">Research</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="algorithms.html">Algorithms</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="design.html">Design</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="implementatio.html">Implementation</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="testin.html">Testing</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="evaluation.html">Evaluation</a>
            </li>
            <li class="submenu" class="submenu">
                <a class="mainlink" href="appendix.html">Appendix</a>
            </li>
        </ul>
        <a class='menu-mobile'>
            <span>Menu</span>
        </a>
        <!-- ***** Menu Start ***** -->
    </header>

    <!-- the first re-usable section with topic and contents -->
    <section class="section" id="achievements">
        <!-- topic -->
        <h3>Evaluations</h3>
        <!-- contents -->
        <h1>Evaluating QA Models</h1>
        <h5>Common Metrics to Evaluate QA Models</h5>
        <p align="justify">There are two dominant metrics used by many question answering datasets, including SQuAD: exact match (EM) and F1 score. These scores are computed on individual question+answer pairs. When multiple correct answers are possible for a given question, the maximum score over all possible correct answers is computed. Overall EM and F1 scores are computed for a model by averaging over the individual example scores.
        </p>

        <h5>Exact Match</h5>
        <p align="justify">For each question+answer pair, if the characters of the model's prediction exactly match the characters of (one of) the True Answer(s), EM = 1, otherwise EM = 0. This is a strict all-or-nothing metric; being off by a single character results in a score of 0. When assessing against a negative example, if the model predicts any text at all, it automatically receives a 0 for that example.
        </p>

        <h5>F1</h5>
        <p align="justify">F1 score is a common metric for classification problems, and widely used in QA. It is appropriate when we care equally about precision and recall. In this case, it's computed over the individual words in the prediction against those in the True Answer. The number of shared words between the prediction and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth. 
        </p>

        <h4>Datasets Used</h4>
        <p align="justify">The Stanford Question Answering Dataset (SQuAD) is a set of question and answer pairs that present a strong challenge for NLP models. Due to its size (100,000+ questions), its difficulty due to the model only has access to a single passage and the fact that its answers are more complex and thus require more-intensive reasoning, SQuAD is an excellent dataset to train NLP models on.
        <br>
        SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. 
        <br>
        We have decided to choose a model that has been pre-trained on SQuAD2.0 for improved performance.
        </p>

        <h4>Our Evaluations</h4>
        <p align="justify">We shortlisted 5 well performing pre-trained QA models from Hugging Face and analysed them using their f1 and EM values on the SQuAD2.0 dataset, which are as follows:</p>
        <br>
        <table>
            <tr>
              <th>Model Name</th>
              <th>F1</th>
              <th>EM</th>
            </tr>
            <tr>
             <td>distilbert-base-cased-distilled-squad</td>
             <td>86.996</td>
             <td>79.600</td>
            </tr>
            <tr>
              <td>deepset/roberta-base-squad2</td>
              <td>82.950</td>
              <td>79.931</td>
            </tr>
            <tr>
                <td>deepset/minilm-uncased-squad2</td>
                <td>79.548</td>
                <td>76.192</td>
            </tr>
            <tr>
                <td>bert-large-uncased-whole-word-masking-finetuned-squad</td>
                <td>83.876</td>
                <td>80.885</td>
            </tr>
            <tr>
                <td>deepset/bert-base-cased-squad2</td>
                <td>74.671</td>
                <td>71.152</td>
            </tr>
        </table>
        <br>
        <p>We also ran all models on a sample document provided by our client, asked the models questions related to our requirements, tabulated the results and asked our peers to rate the sensibility of the answers from 0-5 for each of the answers. The results are as follows:</p>
        <b></b>
        <h1>Evaluating spaCy Models</h1>
        <h4>Testing location detection</h4>
        <p>SpaCy has 3 main NLP models: small, medium, and large, which you can choose from to perform a task. The team decided that the large model would be best for our task of extracting all Geopolitical Entities (GPE) from the document. </p>
        <p>We carried out a test on all three models, using the same extract of text, taken from the first two paragraph of the Wikipedia page on the United States. The extracted GPEs were then put into a list as shown below:</p>
        <p>Small model - ['The United States of America', 'U.S.A.', 'USA', 'the United States', 'U.S.', 'US', 'America', 'The United States', 'the Federated States', 'the Marshall Islands', 'the Republic of Palau', 'Canada', 'Mexico', 'Bahamas', 'Cuba', 'Russia', 'Washington', 'D.C.', 'New York City']</p>
        <p>Medium model - ['The United States of America', 'USA', 'the United States', 'U.S.', 'US', 'America', 'The United States', 'the Marshall Islands', 'the Republic of Palau', 'Canada', 'Mexico', 'Bahamas', 'Cuba', 'Russia', 'Washington', 'D.C.', 'New York City']</p>
        <p>Large model - ['The United States of America', 'U.S.A.', 'USA', 'the United States', 'U.S.', 'US', 'America', 'The United States', 'the Federated States of Micronesia', 'the Marshall Islands', 'the Republic of Palau', 'Canada', 'Mexico', 'Bahamas', 'Cuba', 'Russia', 'Washington', 'D.C.', 'New York City']</p>
        <h5>Comments on results</h5>
        <p>Although all three models do a great job of identifying the locations in the text, the large model in one instance was able to recognise “the Federal states of Micronesia” as a GPE, as opposed to the small model that recognised only part of the GPE “the Federal States”, and the medium model not identifying it at all. </p>
        <p>This may not seem like much of an important factor, it would be very important to our clients. Their main requirement was for us to extract precise locations from the DREF documents, for them to correctly identify disasters and issue funds accordingly. As seen in the table above, the large model is significantly larger than the small and medium. However, for reasons stated previously, we prioritised accuracy over any other factor.</p>

        <h4>Model statistics</h4>
        <p>Other metrics we considered when choosing our model, include storage size, F-score, and word vector size. A table comparing the three models using these metrics is shown below: </p>

<!--        TABLE STARTS HERE-->
        <style>
            table, th, td {
                border:1px solid black;
                text-align: center;
                vertical-align: middle;
            }
        </style>
        <table style="width:100%">
            <tr>
                <th> </th>
                <th>Small model</th>
                <th>Medium model</th>
                <th>Large model</th>
            </tr>
            <tr>
                <td>Size</td>
                <td>12 MB</td>
                <td>40 MB</td>
                <td>560 MB</td>
            </tr>
            <tr>
                <td>Word Vector Size</td>
                <td>0 keys, 0 unique vectors (0 dimensions)</td>
                <td>514k keys, 20k unique vectors (300 dimensions)</td>
                <td>514k keys, 514k unique vectors
                    (300 dimensions)
                    /td>
            </tr>
            <tr>
                <td>F-Score</td>
                <td>0.85</td>
                <td>0.85</td>
                <td>0.85</td>
            </tr>
        </table>
<!--        TABLE ENDS HERE HERE-->

        <p>Although all three models have the same F-score, the large model has a much larger vector size compared to the small and large model, making it more accurate.</p>

    </section>


    <section class="section" id="contributions">
        <!-- topic -->
        <h3>Individual Contributions</h3>
        <!-- contents -->
        <img src="assets/images/evaluation/individual_contributions.png" alt="individual contributions table">
    </section>

    <section class="section" id="critical-evaluation">
        <!-- topic -->
        <h3>Critical Evaluation</h3>
        <!-- contents -->
        <h4>User Interface</h4>
        <p>The user interface has been designed similar to Google's search interface. The design is kept simple and clean, and we have received positive feedback from the clients. There could be additional functionality in the UI, such as more search filters
            and advanced search features such as support for boolean logic. The application is usable on mobile devices, however, more work would be needed for a fully responsive UI.
        </p>
        <h4>Stability</h4>
        <p>The application is deployed on the Azure Cloud using a free account. This limits the amount of cloud credit available, giving us little computing resources. We assume that this is one cause for the relatively slow performance of our deployed application.
            However, we have not experienced any crashes. This indicates relatively high stability.
        </p>
        <h4>Efficiency</h4>
        <p>
            We are using Elasticsearch for the search engine. This is a highly optimised framework for search tasks, offering high performance for any search algorithm. After having optimised the API calls to the X5GON API service, which were an efficiency bottleneck
            before, our application has delivered highly efficient search performance.
        </p>
        <h4>Compatibility</h4>
        <p>
            We have not been able to optimise our application for mobile devices, as mentioned above. Nonetheless, the application is deployed as a cloud-based web application on Azure. This offers performance and compatibility independent of the user's operating
            system. It is thus compatible with Windows, Linux and Mac, as well as mobile operating systems for smartphones and tablets.
        </p>
        <h4>Maintainability</h4>
        <p>
            The code has been written with software engineering practices in mind. The architecture clearly separates the project into frontend and backend. There are only a few API calls that the frontend (view) uses to communicate with the backend (controller).
            The controller then allocates tasks to the relevant models. This allows future developers to expand the system with additional views, such as a mobile application, as well as more search models.
        </p>
        <h4>Project management</h4>
        <p>
            We employed an agile approach to software engineering in our project. By defining 2-week sprints with clear objectives, we were able to respond to feedback from the clients. Our central team management platform was Notion. We collected meeting notes there
            such that everyone was on the same page. It also acted as a central store for key documents, such as the requirements, technologies and designs. Weekly team syncs were used to discuss progress and blockers, as well as to set tasks for each
            team member. This worked well for keeping everyone involved and making continuous progress with the project. Reflecting on the team effort throughout, there was a noticeable increase in effort towards the end of the project. This could have
            been prevented by a more disciplined work ethic in the earlier stages of the project, where we made less progress than what we could have done.
        </p>
    </section>

    <section class="section" id="future-work">
        <!-- topic -->
        <h3>Future Work</h3>
        <!-- contents -->
        <p>
            The application is far from complete, there are many more features that could be implemented.
            One issue with the current application is that it does not use HTTPS.
            Since it is not deployed on a public domain, we could not apply for an SSL certificate.
            Another issue is that in the X5GON database there are many documents that have the same title but different document IDs.
            These are different parts of the same lecture series so it would make sense to group them together in one search result.
            This would make the search results more concise, allowing the user to see more unique results at a glance.
            Our clients also advised that they would like us to add pagination to the search results, however, we were unable to implement this due to a lack of time before the project submission.
            <br>
            <br>
            One particularly relevant part of the unfinished requirements is the part about modelling knowledge.
            Requirement 26 is "Show difficulty estimations of the documents to users (readability score)" and requirement 27 is "Estimate the current knowledge level of the user using their learning history".
            Combining these two would allow the application to go beyond being a simple search engine, and more towards a recommendation system.
            A learning platform that adapts materials to the user's knowledge could revolutionise the way we learn online.
        </p>
    </section>
    <footer class="footer-content">
        <div class="row">
            <div class="right-content">
                <h2>About Us</h2>
                <p>
                    We're an ambitious student team from UCL Computer Science.
                </p>
                <ul class="social">
                    <li><a href="#"><i class="fa fa-github"></i></a></li>
                </ul>
            </div>
            <div class="left-content">
                <h2>Site Menu</h2>
                <div class="container">
                    <div class="nav-list">
                        <div class="col-3"><a href="index.html">Home</a></div>
                        <div class="col-3"><a href="requirements.html">Requirement</a></div>
                        <div class="col-3"><a href="research.html">Research</a></div>
                        <div class="col-3"><a href="algorithm.html">Algorithms</a></div>
                        <div class="col-3"><a href="design.html">Design</a></div>
                        <div class="col-3"><a href="implementatio.html">Implementation</a></div>
                        <div class="col-3"><a href="testin.html">Testing</a></div>
                        <div class="col-3"><a href="evaluation.html">Evaluation</a></div>
                        <div class="col-3"><a href="appendix.html">Appendix</a></div>
                    </div>
                </div>

            </div>
        </div>
        <div class="sub-footer">
            <p>Copyright &copy; 2021 X5GON</p>
        </div>
    </footer>

</body> 